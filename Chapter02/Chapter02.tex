\chapter{Fundamental Concepts}
\section{Breakdown of Classical Physics}
The necessity for a departure from
classical mechanics is clearly demonstrated by:
\begin{enumerate}
\item {\sf The anomalous stability of atoms and molecules:} According to classical
physics, an electron orbiting a nucleus should lose energy by emission
of synchrotron radiation, and gradually spiral in towards the nucleus. Experimentally,
this is not observed to happen.
\item {\sf The anomalously low specific heats of atoms and molecules:} According to
the equipartition theorem of classical physics, each degree of freedom of
an atomic or molecular system should contribute $R/2$ to its molar specific heat, where $R$ is the ideal gas constant.
In fact, only the translational and some rotational degrees of freedom seem
to contribute. The vibrational degrees of freedom appear to make no contribution
at all
(except at high temperatures). Incidentally, this fundamental 
problem with classical physics was known and appreciated in the middle of the
nineteenth century. Stories that physicists at the start of the twentieth century thought that
classical physics explained everything, and that there was nothing left to
discover, are largely apocryphal (see  Feynman, Vol.~I, Cha.~40).
\item {\sf The ultraviolet catastrophe:} According to classical physics, the energy
density of an electromagnetic field in vacuum is infinite due to a divergence of
energy carried by short wave-length modes. Experimentally, there is no such
divergence,  and the total energy density is finite. 
\item {\sf Wave-particle duality:} Classical physics can deal with waves {\em or}
particles. However, various experiments ({\em e.g.}, light interference, the photo-electric effect,
electron diffraction) show quite clearly that waves sometimes act as if they
were streams of particles, and streams of particles sometimes act as if they 
were waves. This is completely inexplicable within the framework of
classical physics. 
\end{enumerate}

\section{Polarization of Photons}
It is known experimentally that when plane polarized light is used to
eject photo-electrons there is a preferred direction of emission of
the electrons. Clearly, the polarization properties of light, which are more usually
associated with its wave-like behaviour, also extend to its  particle-like
behaviour. In particular, a polarization can be ascribed to each 
individual photon in a
beam of light. 

Consider the following well-known experiment. A beam of plane polarized
light is passed through a polaroid film, which
has the property that it is only transparent to
light whose plane of polarization lies perpendicular to its optic axis.
  Classical electromagnetic wave theory tells us that
if the beam is polarized perpendicular to the optic axis then all of the light
is transmitted, if the beam is polarized parallel to the optic axis
then none of the light is transmitted, and if the light is
polarized at an angle $\alpha$ to the  axis then  a fraction $\sin^2\alpha$
of the beam is transmitted. Let us try to account for these observations
at the individual photon level.  

A beam of light which is plane polarized in a certain direction is
made up of a stream of photons which are each plane polarized in that direction.
This picture leads to no difficulty if the plane of polarization lies parallel
or perpendicular to the optic axis of the polaroid. In the former case, none of the
photons are transmitted, and, in the latter case, all of the photons are transmitted.
But, what happens in the case of an obliquely polarized incident beam?

The above question is not very precise. Let us reformulate it as a question
relating to the result of some experiment which we could perform. 
Suppose that we were to fire a single photon at a polaroid film, and then look
to see whether or not it emerges from the other side. The possible
results of the experiment are that either a whole photon, whose energy
is equal to the energy of the incident photon, is observed, or no photon
is observed. Any photon which is transmitted though the film must 
be polarized perpendicular to the optic axis. Furthermore, it is impossible to
imagine (in physics) finding part of a photon on the other side of the film.
If we repeat the experiment a great number of times then, on average,
a fraction $\sin^2\alpha$ of the photons are transmitted through the film, and
a fraction $\cos^2\alpha$ are absorbed. Thus, we conclude that a photon has a {\em probability}
$\sin^2\alpha$ of being transmitted as a photon polarized in the plane
perpendicular to the optic axis, and a probability $\cos^2\alpha$ of
being absorbed. These values for the probabilities lead to the correct classical
limit for a beam containing a large number of photons. 

Note that we have only been able to preserve the individuality of photons,
in all cases, by abandoning the determinacy of classical theory, and
adopting a fundamentally {\em probabilistic} approach. We have no
way of knowing whether an individual obliquely polarized photon is going to
be absorbed by or transmitted through a polaroid film. We only know the
probability of each event occurring. This is a fairly sweeping statement, but
recall that the state of a photon is fully specified once its energy, direction
of propagation, and polarization are known. If we imagine performing experiments
using monochromatic light, normally incident on a polaroid film, with a
particular oblique polarization, then the state of each individual photon
in the beam is completely specified, and there is nothing left over to
uniquely
determine whether the photon is transmitted or absorbed by the film. 

The above discussion about the results of an experiment with a single
obliquely polarized photon incident on a polaroid film answers all that can
be legitimately asked about what happens to the photon when it reaches the
film. Questions as to what decides whether the photon is transmitted
or not, or how it changes its direction of polarization, are illegitimate, since
they do not relate to the outcome of a possible experiment. Nevertheless, some further description
is needed in order to allow the results of this experiment to be correlated
with the results of other experiments which can be performed using photons. 

The further description provided by quantum mechanics is as follows. It is
supposed that a photon polarized obliquely to the optic axis can be regarded 
as being partly in a state of polarization parallel to the axis, and partly
in a state of polarization perpendicular to the axis. In other words, the 
oblique polarization state is some sort of {\em superposition} of two
states of parallel and perpendicular polarization. Since there is nothing special
about the orientation of the optic axis in our experiment, we must conclude
that any state of polarization can be regarded as a superposition of two
mutually perpendicular states of polarization. 

When we make the photon encounter a polaroid film, we are subjecting it to
an observation. In fact, we are observing whether it is polarized 
parallel or perpendicular to the optic axis. The effect of making this observation
is to force the photon entirely into a state of parallel or perpendicular
polarization. In other words, the photon has to jump suddenly from being partly
in each of these two states to being entirely in one or the other of them. Which
of the two states it will jump into cannot be predicted, but is governed
by probability laws. If the photon jumps into a state of parallel polarization then it
is absorbed. Otherwise, it is transmitted. Note that, in this example, the introduction
of 
indeterminacy into the problem  is clearly connected with the act of
observation.  In other words, the indeterminacy is related to the
inevitable disturbance of the system associated with the act of observation. 

\section{Fundamental Principles of Quantum Mechanics}
There is nothing special about the transmission and absorption of photons
through a polaroid film. Exactly the same conclusions as those outlined above
are obtained by studying other simple experiments, such as the interference of photons
(see Dirac, Sect.~I.3), and the Stern-Gerlach experiment (see Sakurai,  
Cha.~1; Feynman, Cha.~5). The study of
these simple experiments leads us to formulate the following fundamental principles of
quantum mechanics:
\begin{enumerate}
\item {\sf Dirac's razor:} Quantum mechanics can only answer questions regarding the
outcome of possible experiments. Any other questions lie beyond the realms of
physics.
\item {\sf The principle of superposition of states:} Any  microscopic
system ({\em i.e.}, an atom, molecule, or particle)  in a given state can be regarded
as being partly in each of two or more other states. In other words, any state
can be regarded as a superposition of two or more other states. Such superpositions can be performed in an infinite number
of different ways.
\item {\sf The principle of indeterminacy:} An observation made on a 
microscopic system causes it
to jump into one or more particular states (which are related to the type
of observation). It is impossible to predict into which final 
state a particular system
will jump, however the probability of a given  system jumping into a given final
state can be predicted. 
\end{enumerate}
The first of these principles was formulated by quantum physicists (such as Dirac)
in the 1920s
 to fend off awkward questions such as ``How can a system suddenly
jump from one state into another?'', 
or ``How does a system decide which state to
jump into?''. As we shall see, the second principle is
the basis for the mathematical formulation of quantum mechanics.
The final principle is still rather vague. We need to extend it 
so that we can predict which possible states  a system can jump into after
a particular type of observation, as well as the probability of
the system making a particular jump.

\section{Ket Space}
Consider a microscopic system composed of particles or bodies with
specific properties (mass, moment of inertia, {\em etc.})\ interacting according
to specific laws of force. There will be various possible motions of the
particles or bodies consistent with the laws of force. Let us term each such
motion a {\em state} of the system. According to the principle of superposition
of states, any given state can be regarded as a superposition of two or more other
states. Thus, states must be related to mathematical quantities of a kind which
can be added together to give other quantities of the same kind. The most
obvious examples of such quantities are {\em vectors}. 

Let us consider a particular microscopic system in a particular state, which we
label $A$: {\em e.g.}, a photon with a particular energy, momentum, and polarization.
We can represent this state as a particular vector, which we also
label $A$, residing in some vector space, where the other elements of the space
represent all of the other possible states of the system. Such a space
is called a {\em ket space} (after Dirac). The state vector $A$ is
conventionally written
\begin{equation}
|A\rangle.
\end{equation}
Suppose that state $A$ is, in fact, the superposition of two different states,
$B$ and $C$. This interrelation is represented in ket space by writing 
\begin{equation}
|A\rangle = |B\rangle + |C\rangle,
\end{equation}
where $|B\rangle$ is the vector relating to the state $B$, {\em etc}. For instance, state
$|B\rangle$ might represent a photon propagating
in the $z$-direction, and plane polarized in the $x$-direction, and state
$|C\rangle$ might represent a similar  photon plane polarized in the $y$-direction. 
In this case,
the sum of these two states represents a photon whose plane of polarization
makes an angle of $45^\circ$ with both the $x$- and $y$-directions (by analogy
with classical physics). This latter state is represented by $|B\rangle+|C\rangle$ in ket
space. 

Suppose that we want to construct a state whose plane of polarization makes
an arbitrary angle  $\alpha$ with  the $x$-direction. We can do this 
via a suitably weighted superposition of states $B$ and $C$. By analogy
with classical physics, we require $\cos\alpha$ of state $B$, and $\sin\alpha$
of state $C$. This new state is represented by
\begin{equation}\label{e2.3}
\cos\alpha\,|B\rangle + \sin\alpha\,|C\rangle
\end{equation}
in ket space. Note that we cannot form a new state by superposing a state
with itself. For instance, a photon polarized in the $y$-direction 
superposed with another photon polarized in the $y$-direction (with the
same energy and momentum) gives 
the same  photon. This implies
that the ket vector
\begin{equation}
c_1 |A\rangle + c_2 |A\rangle = (c_1 + c_2) |A\rangle 
\end{equation}
corresponds to the same state that $|A\rangle$ does. Thus, ket vectors differ
from conventional vectors in that  their magnitudes, or lengths, are physically
irrelevant. All the states of the system are in one to one correspondence
with all the possible directions of vectors in the ket space, no distinction
being made between the directions of the ket vectors $|A\rangle$ and $-|A\rangle$.
There is, however, one caveat to the above statements. If $c_1+c_2 = 0$ then the
superposition process yields nothing at all: {\em i.e.}, no state. The absence of
a state is represented by the null vector $|0\rangle$ in ket space. The null vector
has the fairly obvious property that
\begin{equation}
|A\rangle + |0\rangle = |A\rangle,
\end{equation}
for any vector $|A\rangle$. The fact that ket vectors pointing in the same direction
represent the same state relates ultimately to the quantization of  matter: {\em i.e.},
the fact that it comes in irreducible packets called photons, electrons, atoms,
{\em etc.}
If we observe a microscopic   system then we either see a state 
({\em i.e.}, a photon, or an atom, or a molecule, {\em  etc}.)\ or we see nothing---we can never see
a fraction or a multiple 
of a state. In classical physics, if we observe a wave then the amplitude
of the wave can take any value between zero and infinity. Thus, if we were to
 represent a classical wave by a vector, then the magnitude, or length, of the 
vector would correspond to the amplitude of the wave, and the direction
would correspond to the frequency and wave-length, 
 so that two vectors 
of different lengths pointing
in the same direction would represent different wave states. 

We have seen, in Eq.~(\ref{e2.3}),
 that any plane polarized state of a photon can be represented
as a linear superposition of two orthogonal polarization states
in which the weights are real numbers. Suppose that
we want to construct a circularly polarized photon state. Well, we know from
classical physics that a circularly polarized wave is a superposition of two
waves of equal amplitude, plane polarized in orthogonal directions, 
which are in {\em phase quadrature}. This suggests that a circularly polarized photon
is the superposition of a photon polarized in the $x$-direction (state $B$)
and a photon polarized in the $y$-direction (state $C$), with equal weights given
to the two states, but with the proviso that state $C$ is $90^\circ$ out of phase
with state $B$. By analogy with classical physics, we can use {\em complex numbers}
to simultaneously represent the  weighting and relative phase in a
linear superposition. Thus, a circularly polarized photon is represented by
\begin{equation}
|B\rangle + {\rm i}\, |C\rangle
\end{equation}
in ket space. A general elliptically polarized photon is represented by
\begin{equation}
c_1 |B\rangle + c_2 |C\rangle,
\end{equation}
where $c_1$ and $c_2$ are  complex numbers. We conclude that
a ket space
must be a complex vector space if it is to properly 
represent the mutual interrelations
between the possible states of a microscopic system. 

Suppose that the ket $|R\rangle$ is expressible linearly in terms of the kets
$|A\rangle$ and $|B\rangle$, so that
\begin{equation}
|R\rangle = c_1 |A\rangle + c_2 |B\rangle.
\end{equation}
We say that $|R\rangle$ is {\em dependent} on $|A\rangle$ and $|B\rangle$. It follows that
the state $R$ can be regarded as a linear superposition of 
the states $A$ and $B$.
So, we can also say that state $R$ is dependent on states $A$ and $B$. 
In fact, any ket vector (or state) which is expressible linearly in
terms of certain others is said to be dependent on them. Likewise, a
set of ket vectors (or states) are termed {\em independent} if none of
them are expressible linearly in terms of the others.

The dimensionality of a conventional vector space is defined as the number
of independent vectors contained in the space. Likewise, the dimensionality
of a ket space is equivalent to the number of independent ket vectors it contains. 
Thus, the ket space which represents the possible polarization 
states of a photon propagating in the $z$-direction is two-dimensional
(the two independent vectors correspond to photons plane polarized in the
$x$- and $y$-directions, respectively). Some microscopic
systems have a finite number of independent states ({\em e.g.}, the spin states
of an electron in a magnetic field). If there are $N$ independent states,
then the possible states of the 
system are represented as an  $N$-dimensional ket space.  Some microscopic
systems have a denumerably infinite number of independent states ({\em e.g.},
a particle in an infinitely deep, one-dimensional potential well). 
The possible states of such a system are represented as a ket space whose
dimensions are denumerably infinite. Such a space can be treated in more or
less the same manner as a finite-dimensional space. Unfortunately, some
microscopic systems have a nondenumerably infinite number of independent states
({\em e.g.}, a free particle). The possible states  of such a system are represented
as a ket space whose dimensions are nondenumerably infinite. This type of
space requires a slightly different treatment to spaces of finite, or
denumerably infinite, dimensions. 

In conclusion, the  states of a general microscopic system can be  represented
as a complex vector space of (possibly) infinite dimensions. Such a space
is termed a {\em Hilbert space} by mathematicians.

\section{Bra Space}
 A snack machine inputs coins plus some code entered on a key pad, and
(hopefully) outputs a snack. It also does so in a deterministic manner: {\em i.e.},
the same money plus the same code produces the same snack 
(or the same error message) time after time.
Note that the input and output of the machine have completely different natures. 
We can imagine building a rather abstract snack machine which inputs ket
vectors and outputs  complex numbers in a deterministic fashion. Mathematicians
call such a machine a {\em functional}. Imagine a general functional, labeled
$F$, acting on a general ket vector, labeled $A$, and spitting out a general
complex number $\phi_A$. This  process is represented mathematically by writing
\begin{equation}
\langle F|(|A\rangle) = \phi_A.
\end{equation}
Let us narrow our focus to those functionals which preserve the linear dependencies
of the ket vectors upon which they operate. Not surprisingly, such functionals
are termed {\em linear functionals}. A general linear functional, labeled $F$,
satisfies
\begin{equation}\label{e2.10}
\langle F|(|A\rangle + |B\rangle) = \langle F|(|A\rangle) +
 \langle F|(|B \rangle),
\end{equation}
where $|A\rangle$ and $|B\rangle$ are any two kets in a given ket space.

Consider an $N$-dimensional ket space [{\em i.e.}, a finite-dimensional, or
denumerably infinite dimensional ({\em i.e.}, $N\rightarrow\infty$),  space].
Let the $|i\rangle$ (where $i$ runs from 1 to $N$) 
represent $N$ independent ket vectors in this space. 
A general ket vector can be written\footnote{Actually, this is only 
strictly true for finite-dimensional spaces. Only a special subset
of denumerably infinite dimensional spaces have this property ({\em i.e.}, they
are complete), but since a ket space must be complete if it is
to represent the states of a microscopic system, we need only consider
this special subset.}
\begin{equation}\label{e2.11}
|A\rangle = \sum_{i=1}^N \alpha_i |i\rangle,
\end{equation}
where the $\alpha_i$ are an arbitrary set of complex numbers.
The only way the functional $F$ can satisfy Eq.~(\ref{e2.10}) for all vectors in the ket
space is if
\begin{equation}\label{e2.12}
\langle F|(|A\rangle) = \sum_{i=1}^N f_i \,\alpha_i,
\end{equation}
where the $f_i$ are a set of complex numbers relating to the functional.

Let us  define $N$ basis functionals $\langle i|$ which satisfy
\begin{equation}
\langle i|(|j\rangle) = \delta_{ij}.
\end{equation}
It follows from the previous three equations that
\begin{equation}
\langle F| = \sum_{i=1}^N f_i \langle i|.
\end{equation}
But, this implies that the set  of all possible linear functionals acting
on an $N$-dimensional ket space is itself an $N$-dimensional vector space.
This type of vector
 space is called a {\em bra
space} (after Dirac), and its constituent vectors 
(which are actually functionals of the ket space) are called bra vectors.
Note that bra vectors are
quite different in nature to  ket vectors (hence, these
vectors are written in mirror image notation,
$\langle\cdots|$ and $|\cdots \rangle$, so that they can never be confused). 
Bra space is an example of what  mathematicians call a {\em dual
vector space} ({\em i.e.}, it is dual to the original ket space). There is
a one to one correspondence between the elements of the ket space and those
of the related bra space. So, for every element $A$ of the
ket space, there is a corresponding element, which it is also convenient to
label $A$, in the bra space. That is,
\begin{equation}
|A \rangle \stackrel{\rm DC}{\longleftrightarrow}\langle A|,
\end{equation} 
where DC stands for {\em dual correspondence}.

There are an infinite number of ways of setting up the correspondence between
vectors in a ket space and those in the related bra space. However,
only one of these has any physical significance. For a general ket vector
$A$, specified by Eq.~(\ref{e2.11}), the corresponding bra vector is written
\begin{equation}\label{e2.16}
\langle A| = \sum_{i=1}^N \alpha_i^\ast \langle i|,
\end{equation}
where the $\alpha_i^\ast$ are the complex conjugates of the $\alpha_i$.
$\langle A|$ is termed the dual vector to $|A\rangle$. It follows, from the
above, that the dual to $c \langle A|$ is $c^\ast |A \rangle$, where $c$ is
a complex number. More generally,
\begin{equation}
c_1 |A\rangle + c_2 |B\rangle \stackrel{\rm DC}{\longleftrightarrow}
c_1^\ast \langle A| + c_2^\ast \langle B|.
\end{equation}

Recall that a bra vector is a functional which acts on a general ket vector,
and spits out a complex number. Consider the functional which is dual to the
ket vector 
\begin{equation}\label{e2.18}
|B\rangle = \sum_{i=1}^N \beta_i |i\rangle
\end{equation}
acting on the ket vector $|A\rangle$. This
operation is denoted $\langle B|(|A \rangle )$. Note, however, that 
we can omit the round brackets without causing any ambiguity, so the
operation can also be written $\langle B||A \rangle$. This 
expression can be further simplified
 to give $\langle B|A \rangle$. According to Eqs.~(\ref{e2.11}), (\ref{e2.12}), (\ref{e2.16}), 
and (\ref{e2.18}),
\begin{equation}
\langle B|A \rangle = \sum_{i=1}^N \beta_i^\ast \,\alpha_i.
\end{equation}
Mathematicians term $\langle B|A \rangle$ 
 the {\em inner product} of a bra and a ket.\footnote{We 
can now appreciate the elegance of Dirac's 
notation. The combination of a bra and a ket yields a ``bra(c)ket'' (which is
just a number).}
 An inner product is (almost) analogous to a
scalar product between a covariant and contravariant vector in 
some curvilinear space.
It is easily demonstrated that
\begin{equation}\label{e2.20}
\langle B|A \rangle = \langle A|B \rangle^\ast.
\end{equation}
Consider the special case where $|B\rangle \rightarrow |A \rangle$. It
follows from Eqs.~(\ref{e2.12}) and (\ref{e2.20}) 
that $\langle A|A\rangle$ is a real number, and that
\begin{equation}\label{e2.21}
\langle A|A \rangle \geq 0.
\end{equation}
The equality sign only holds if $|A\rangle$ is the null ket [{\em i.e.}, if
all of the $\alpha_i$ are zero in Eq.~(\ref{e2.11})]. This property of bra and ket
vectors is essential for the probabilistic interpretation of quantum mechanics,
as will become apparent later. 

Two kets $|A\rangle$ and $|B \rangle$ are said to be {\em orthogonal}
if 
\begin{equation}
\langle A|B \rangle = 0,
\end{equation}
which also implies that $\langle B|A \rangle=0$.

Given a ket $|A\rangle$ which is not the null ket,
 we can define a {\em normalized ket} $|\tilde{A}\rangle$, where
\begin{equation}
|\tilde{A}\rangle = \left(\frac{1}{\sqrt{\langle A|A \rangle }}\right)|A\rangle,
\end{equation}
with the property
\begin{equation}
\langle \tilde{A}|\tilde{A} \rangle = 1.
\end{equation}
Here, $\sqrt{\langle A|A \rangle }$ is known as the {\em norm} 
or ``length'' of $|A\rangle$, and
is analogous to the length, or magnitude,  of a conventional vector. Since
$|A\rangle$ and $c |A\rangle$ represent the same physical state, it makes sense
to require that all kets corresponding to  physical states have unit norms. 

It is possible to define a dual bra space for a ket space of nondenumerably
infinite dimensions in much the same manner as that described above. The
main differences are that summations over discrete labels become integrations
over continuous labels, Kronecker delta-functions become Dirac delta-functions, 
completeness must be assumed (it cannot be proved), and the normalization convention 
is somewhat different. More of this later. 

\section{Operators}
We have seen that a functional is a machine which inputs a ket vector and
spits out a complex number. Consider a 
somewhat different machine which inputs a ket
vector and spits out another ket vector in a deterministic fashion. Mathematicians
call such a machine an {\em operator}. We are only interested in operators
which preserve the linear dependencies of the ket vectors upon which
they act. Such operators are termed {\em linear operators}. Consider an operator
labeled $X$. Suppose that when this operator acts on a general ket vector
$|A \rangle$ it spits out a new ket vector which is denoted
$X|A\rangle$. Operator $X$ is linear provided that
\begin{equation}
X (|A\rangle + |B \rangle) = X|A\rangle + X|B \rangle,
\end{equation}
for all ket vectors $|A\rangle$ and $|B \rangle$, and
\begin{equation}
X(c |A\rangle) = c X |A\rangle,
\end{equation}
for all complex numbers $c$. Operators $X$ and $Y$ are said to be equal
if
\begin{equation}
X |A\rangle = Y|A\rangle
\end{equation}
for all kets in the ket space in question. Operator $X$ is termed the
{\em null operator}
if
\begin{equation}
X  |A\rangle = |0\rangle
\end{equation}
for all ket vectors in the space. Operators can be added together. Such addition 
is defined to obey a commutative and associate algebra:
\begin{eqnarray}
X + Y &= &Y + X,\\[0.5ex]
X + (Y + Z) &=& (X + Y ) + Z.
\end{eqnarray}
Operators can also be multiplied. The multiplication is associative:
\begin{eqnarray}
X(Y|A\rangle)& =& (X\,Y) |A\rangle = X\,Y| A \rangle,\\[0.5ex]
X(Y\,Z)& = &(X\,Y) Z = X\,Y\,Z.
\end{eqnarray}
However, in general, it is {\em noncommutative}:
\begin{equation}
X\,Y \neq Y\,X.
\end{equation}

So far, we have only considered linear operators acting on ket vectors. We can also
give a meaning to their operating on bra vectors. Consider the inner product
of a general bra $\langle B|$ with the ket $X|A\rangle$. This product is a
number which depends linearly on $|A\rangle$. Thus, it may be considered to
be the inner product of $|A\rangle$ with some bra. This bra depends linearly
on $\langle B|$, so we may look on it as the result of some linear
operator applied to $\langle B|$. This operator is uniquely determined by the
original operator $X$, so we might as well call it the same operator acting on
$|B\rangle$. A suitable notation to use for the resulting bra when $X$ operates on
$\langle B|$ is $\langle B|X$. The equation which defines this
vector is
\begin{equation}\label{e2.32}
(\langle B|X) |A\rangle = \langle B|( X|A\rangle)
\end{equation}
for any $|A\rangle$ and $\langle B|$. 
The triple product of $\langle B|$, $X$, and $|A\rangle$
can be written $\langle B|X| A \rangle$ without ambiguity, provided we adopt the
convention that the bra vector always goes on the left, the operator in the middle,
and the ket vector on the right. 

Consider the dual bra to $X|A\rangle$. This bra depends antilinearly on
$|A\rangle$ and must therefore depend linearly on $\langle A|$. 
Thus, it may 
be regarded as the result of some linear operator applied to $\langle A|$.
This operator is termed the {\em adjoint} of $X$, and is denoted $X^{\dag}$. Thus,
\begin{equation}\label{e2.33}
X | A\rangle \stackrel{\rm DC}{\longleftrightarrow} \langle A|X^{\dag}.
\end{equation}
It is readily demonstrated that
\begin{equation}
\langle B|X^{\dag}|A\rangle = \langle A|X |B\rangle^\ast,
\end{equation}
plus
\begin{equation}
(X\, Y)^{\dag} = Y^{\dag}\, X^{\dag}.
\end{equation}
It is also easily seen that the adjoint of the adjoint of a linear operator
is equivalent to the original operator. A {\em Hermitian} operator $\xi$ has
the special property that it is its own adjoint:  {\em i.e.},
\begin{equation}
\xi= \xi^{\dag}.
\end{equation}

\section{Outer Product}
So far we have formed the following products: $\langle B| A \rangle$, $X| A \rangle$,
$\langle A| X$, $X\,Y$, $\langle B| X | A\rangle$. Are there any other products
we are allowed to form? How about
\begin{equation}\label{e2.37}
|B \rangle \langle A|~?
\end{equation}
This clearly depends linearly on the ket $|A\rangle$ and the bra $|B\rangle$.
Suppose that we right-multiply the above product by the general ket
 $|C \rangle$.
We obtain 
\begin{equation}
|B \rangle \langle A | C\rangle = \langle A | C\rangle |B \rangle,
\end{equation}
since $\langle A | C\rangle$ is just a number. Thus, $|B\rangle \langle A|$ acting
on a general ket $|C\rangle$ yields another ket. Clearly, the product 
$|B\rangle \langle A|$ is a linear  operator. This operator also acts on bras,
as is easily demonstrated by left-multiplying the expression (\ref{e2.37}) by a general
bra $\langle C|$. It is also easily demonstrated that
\begin{equation}
(|B \rangle \langle A|)^{\dag} = |A \rangle \langle B|.
\end{equation}
Mathematicians term the operator $|B \rangle \langle A|$ the {\em outer product}
of $|B \rangle$ and $\langle A|$. The outer product should not be confused with
the inner product, $\langle A|B \rangle$, which is just a number. 

\section{Eigenvalues and Eigenvectors}\label{s2.8}
In general, the ket $X|A\rangle$ is not a constant multiple of $|A\rangle$.
However, there are some special kets 
 known as the {\em eigenkets} of operator $X$. These are denoted
\begin{equation}
|x'\rangle, |x''\rangle, |x'''\rangle \ldots,
\end{equation}
and have the property 
\begin{equation}
X|x'\rangle = x'|x'\rangle,~~~X|x''\rangle = x''|x''\rangle \dots,
\end{equation}
where $x'$, $x''$, $\ldots$ are  numbers called
{\em eigenvalues}. Clearly, applying $X$ to one of its
eigenkets yields the same eigenket multiplied by the associated eigenvalue.

Consider the eigenkets and eigenvalues of a Hermitian operator $\xi$. These are
denoted
\begin{equation}\label{e2.42}
\xi |\xi'\rangle = \xi' |\xi' \rangle,
\end{equation}
where $|\xi'\rangle$ is the eigenket associated with the eigenvalue $\xi'$.
Three important results are readily deduced:

{\em (i) The eigenvalues are all real numbers, and the eigenkets corresponding
to different eigenvalues are orthogonal.}
Since $\xi$ is Hermitian, the dual equation to Eq.~(\ref{e2.42}) (for the eigenvalue
$\xi''$) reads
\begin{equation}
\langle \xi''|\xi = \xi''^\ast \langle \xi''|.
\end{equation}
If we left-multiply Eq.~(\ref{e2.42}) by $\langle \xi''|$, right-multiply the above
equation by $|\xi'\rangle$, and take the difference, we obtain
\begin{equation}
(\xi' - \xi''^\ast) \langle \xi''|\xi'\rangle = 0.
\end{equation}
Suppose that the eigenvalues $\xi'$ and $\xi''$ are the same. It follows from the
above that
\begin{equation}
\xi' = \xi'^\ast,
\end{equation}
where we have used the fact that $|\xi'\rangle$ is not the null ket. This proves
that the eigenvalues are real numbers. Suppose that the eigenvalues
$\xi'$ and $\xi''$ are different. It follows that
\begin{equation}
\langle \xi''|\xi'\rangle = 0,
\end{equation}
which demonstrates that eigenkets corresponding to different eigenvalues are
 orthogonal.


{\em (ii) The eigenvalues associated with eigenkets are the same as the eigenvalues
associated with eigenbras.} An eigenbra of $\xi$ corresponding to an eigenvalue
$\xi'$ is defined
\begin{equation}
\langle \xi'|\xi = \langle \xi'|\xi'.
\end{equation}

{\em (iii) The dual of any eigenket is an eigenbra belonging to the same eigenvalue,
and conversely.}


\section{Observables}
We have developed a mathematical formalism which comprises three types of objects---bras, kets, and linear operators. We have already seen that kets can be used to
represent the possible states of a microscopic system. 
However, there is a one
to one correspondence between the elements of a ket space and its dual bra space,
so we must conclude that bras could just as well be used to represent the states of
a microscopic system. What about the dynamical variables of the  system
({\em e.g.}, its position, momentum, energy, spin, {\em etc}.)? How can these be represented in
our formalism? Well, the only objects we have left over are operators.
We, therefore, assume that {\em the dynamical variables of a microscopic system
are represented as linear operators acting on the bras and kets  which correspond 
to
the various possible states of the system}. Note that the operators have to be linear,
otherwise they would, in general, spit out bras/kets pointing in different directions
when fed bras/kets pointing in the same direction but differing in length. Since the
lengths of bras and kets have no physical significance, it is reasonable to suppose
that non-linear operators are also without physical significance. 

We have seen that if we observe the polarization state of a photon, by
placing a polaroid film in its path, the result is to cause the photon to
jump into a state of polarization parallel or perpendicular to the optic
axis of the film. The former state is absorbed, and the latter state is
transmitted (which is how we tell them apart). In general, we cannot predict
into which state a given photon will jump (except in a statistical sense). 
However, we do know that if the photon is initially polarized parallel to
the optic axis then it will definitely be absorbed, and if it is
initially  polarized
perpendicular to the axis then it will definitely be transmitted. 
We also known that after passing though the film a photon must be 
in a state of polarization perpendicular to the optic axis (otherwise
it would not have been transmitted). We can make a second observation of
the polarization state of such a photon by placing an identical polaroid
film (with the same orientation of the optic axis) immediately behind the
first film. It is clear that the photon will definitely be transmitted
through the second film. 

There is nothing special about the polarization states of a photon. So, more
generally, we can say that when  a dynamical variable of
a microscopic system is measured the  system is caused to jump into one of a number of
{\em independent} states (note that the perpendicular and parallel polarization
states of our photon are linearly independent). In general, each of these final states
is associated with a different result of the measurement:
{\em i.e.}, a different value of the dynamical variable. Note that the result
of the measurement must be a {\em real} number (there are no measurement machines
which output complex numbers). Finally, if an observation is made, and the system
is found to be a one particular final state, with one particular value for the
dynamical variable, then a second observation, made immediately after the first one,
will {\em definitely} find the system in the same state, and yield the same
value for the dynamical variable.

How can we represent all of these facts in our mathematical formalism? Well,
by a fairly non-obvious leap of intuition, we are going to assert that
{\em a measurement of a dynamical variable corresponding to an operator $X$ in ket
space causes the system to jump into a state corresponding to one of the
eigenkets of $X$}. Not surprisingly, such a state is termed an {\em eigenstate}. 
Furthermore,
 {\em the result of the measurement is the eigenvalue associated with the eigenket
into which the system jumps}. The fact that the result of the measurement must
be a real number implies that {\em dynamical variables can only be represented
by Hermitian operators} (since only Hermitian operators are guaranteed to have
real eigenvalues). The fact that the eigenkets of a Hermitian operator
corresponding to different eigenvalues ({\em i.e.}, different results of the
measurement) are orthogonal is in accordance with our earlier requirement that the
states into which the system jumps should be mutually independent. 
We can conclude that the result of  a measurement of a dynamical variable
represented by a Hermitian operator $\xi$ must be one of the eigenvalues
of $\xi$. Conversely, every eigenvalue of $\xi$ is a possible result
of a measurement made on the corresponding dynamical variable. This gives
us the physical significance of the eigenvalues. (From now on, the distinction
between a state and its representative ket vector, and a dynamical variable
and its representative operator, will be dropped, for the sake of
simplicity.)

It is reasonable to suppose that {\em if a certain dynamical variable
$\xi$ is measured with the system in a particular state, then the states
into which the system may jump on account of the measurement are such
that the original state is dependent on them.} This fairly innocuous
statement has two very important corollaries. First, immediately after an
observation whose result is a particular eigenvalue $\xi'$,  the system
is left in the associated eigenstate. However, this eigenstate is 
orthogonal to ({\em i.e.}, independent of) any other eigenstate corresponding
to a different eigenvalue. It follows that a second measurement made
immediately after the first one must leave the system in an eigenstate
corresponding to the eigenvalue $\xi'$. In other words, the second measurement is
bound to give the same result as the first. Furthermore, {\em if the system is
in an eigenstate of $\xi$, corresponding to an eigenvalue $\xi'$, then
a measurement of $\xi$ is bound to give the result $\xi'$}. This follows
because the system cannot jump into an eigenstate corresponding to a
different eigenvalue of $\xi$, since such a state is not dependent on
the original state. Second, it stands to reason that a measurement
of $\xi$ must always yield some result. It follows that no matter what
the initial state of the system, it must always be able to jump into one
of the eigenstates of $\xi$. In other words, a general ket must  always
be dependent on the eigenkets of $\xi$. This can  only be the case if the eigenkets
form a {\em complete set} ({\em i.e.}, they span ket space). Thus, {\em in order for
a Hermitian operator $\xi$ to be observable its eigenkets must form a complete set}.
A Hermitian  operator which satisfies this condition is termed an {\em observable}. 
Conversely, any observable quantity must be a Hermitian operator with a
complete set of eigenstates.

\section{Measurements}\label{s2.10}
We have seen that a measurement of some observable $\xi$ of a microscopic
system causes the system to jump into one of the eigenstates of $\xi$. The
result of the measurement is the associated eigenvalue (or some function of
this quantity). It is impossible to determine into which eigenstate a 
given system will jump, but it is possible to predict the probability of
such a transition. So, what is the probability that a system in some
initial state $|A\rangle$ makes a transition to an eigenstate $|\xi'\rangle$
of an observable $\xi$, as a result of a measurement made on the system? 
Let us start with the simplest case. If the system is initially in an eigenstate
$|\xi'\rangle$ then the transition probability to a eigenstate $|\xi''\rangle$
corresponding to a different eigenvalue is zero, and the transition probability
to the same eigenstate $|\xi'\rangle$ is unity. It is convenient to normalize 
our eigenkets such that they all have unit norms. It follows from the
orthogonality property of the eigenkets that
\begin{equation}\label{e2.48}
\langle \xi'|\xi''\rangle = \delta_{\xi' \xi''},
\end{equation}
where $ \delta_{\xi' \xi''}$ is unity if $\xi'=\xi''$, and zero otherwise.
For the moment, we are assuming that  the eigenvalues of $\xi$ 
 are all different.

Note that the probability of a transition from an initial eigenstate
$|\xi'\rangle$ to a final eigenstate $|\xi''\rangle$ is the same as
the value of the  inner product
$\langle \xi'|\xi''\rangle$. Can we use  this correspondence 
to obtain  a general rule for
calculating transition probabilities? Well, suppose that the system is initially
in a state $|A\rangle$ which is not an eigenstate of $\xi$. Can we
identify the transition probability to a final eigenstate $|\xi'\rangle$ with
the inner product $\langle A| \xi'\rangle$? The straight answer is ``no'',
since  $\langle A| \xi'\rangle$ is, in general, a complex number, and complex
probabilities do not make much sense. Let us try again. How about if we
identify the transition probability with the {\em modulus squared} of the
inner product, $|\langle A| \xi'\rangle|^2$\,? This 
quantity is definitely a positive
number (so it could be a probability). This guess also gives the right answer for
the transition probabilities between eigenstates. In fact, it is the
correct guess.

Since the eigenstates of an observable $\xi$  form a complete
set, we can express any given state $|A\rangle$ as a linear combination of them.
It is easily demonstrated that
\begin{eqnarray}\label{e2.49a}
|A\rangle &= &\sum_{\xi'} |\xi'\rangle \langle\xi'|A\rangle,\\[0.5ex]
\langle A|&= &\sum_{\xi'} \langle A|\xi'\rangle \langle\xi'|,\\[0.5ex]
\langle A|A\rangle&=& \sum_{\xi'} \langle A|\xi'\rangle
 \langle\xi'|A\rangle 
= \sum_{\xi'}|\langle A|\xi' \rangle|^2,\label{e2.49c}
\end{eqnarray}
where the summation is over all the different eigenvalues of
$\xi$, and use has been
made of Eq.~(\ref{e2.20}), and
the fact that the eigenstates are mutually  orthogonal. Note that all of the
above results follow from the extremely useful (and easily proved) result
\begin{equation}\label{e2.50}
\sum_{\xi'} |\xi'\rangle \langle \xi'| = 1,
\end{equation}
where 1 denotes the identity operator.
The relative
probability of a transition to an eigenstate $|\xi'\rangle$, which is
equivalent to 
the relative probability of a measurement of $\xi$ yielding the result $\xi'$, is
\begin{equation}
P(\xi') \propto |\langle A| \xi'\rangle|^2.
\end{equation}
The absolute probability 
is clearly
\begin{equation}
P(\xi') = \frac{ |\langle A| \xi'\rangle|^2}{\sum_{\xi'}|\langle A| \xi'\rangle|^2}
=\frac{|\langle A| \xi'\rangle|^2}{\langle A|A\rangle}.
\end{equation}
If the ket  $|A\rangle$ is normalized such that its norm is unity,  then this
probability simply reduces to
\begin{equation}
P(\xi')= |\langle A| \xi'\rangle|^2.
\end{equation}

\section{Expectation Values}
Consider an {\em ensemble} of microscopic systems  prepared in the
same initial state $|A\rangle$. Suppose a measurement of the observable $\xi$ is
made on each  system. We know that each  measurement yields the value $\xi'$ with
probability $P(\xi')$. What is the mean value of the measurement? This quantity,
which is generally referred to as the {\em expectation value} of $\xi$, is
given by
\begin{eqnarray}
\langle\xi\rangle &=&
 \sum_{\xi'} \xi' P(\xi') = \sum_{\xi'} \xi'|\langle A|\xi'\rangle|^2\nonumber
\\[0.5ex]
&=& \sum_{\xi'} \xi' \langle A|\xi'\rangle \langle \xi' | A\rangle
= \sum_{\xi'} \langle A|\xi|\xi'\rangle \langle \xi'|A\rangle,
\end{eqnarray}
which reduces to 
\begin{equation}
\langle\xi\rangle = \langle A|\xi|A\rangle
\end{equation}
with the aid of Eq.~(\ref{e2.50}). 

Consider the identity operator, 1. All states are eigenstates of this operator
with the eigenvalue unity. Thus, the expectation value of this operator
is always unity: {\em i.e.},
\begin{equation}\label{e2.56}
\langle A|1|A\rangle = \langle A|A\rangle =1,
\end{equation}
for all $|A\rangle$. Note that it is only possible to normalize a given
ket $|A\rangle$ such that Eq.~(\ref{e2.56}) is satisfied because of the more general
property (\ref{e2.21}) of the norm. This property depends on the particular correspondence
(\ref{e2.16}), that we adopted earlier, between the elements of a ket space and those of its
dual bra space. 

\section{Degeneracy}
Suppose that two different eigenstates $|\xi'_a\rangle$ and $|\xi'_b\rangle$ 
of $\xi$ 
correspond to the {\em same}\/ eigenvalue $\xi'$. These are termed {\em degenerate}
eigenstates. Degenerate eigenstates are 
necessarily orthogonal to any eigenstates corresponding to
different eigenvalues, but, in general, they are not orthogonal to each other
({\em i.e.}, the proof of orthogonality given in Sect.~\ref{s2.8} does not work in this case).
This is unfortunate, since much of the previous formalism depends crucially
on the mutual orthogonality of the different eigenstates
of an observable. Note, however, that
any linear combination of $|\xi'_a\rangle$ and $|\xi'_b\rangle$ is also
an eigenstate corresponding to the eigenvalue $\xi'$. It follows that we
can always {\em construct} two mutually orthogonal degenerate eigenstates.
For instance,
\begin{eqnarray}
|\xi_1'\rangle &=& |\xi_a'\rangle,\\[0.5ex]
|\xi_2'\rangle&=&\frac{ |\xi_b'\rangle - \langle \xi_a'|\xi_b'\rangle
|\xi_a'\rangle}{1-|\langle \xi_a'|\xi_b'\rangle|^2}.
\end{eqnarray}
This result is easily generalized to the case of more than two degenerate eigenstates.
We conclude that it is always possible to construct a complete set of mutually
orthogonal eigenstates for any given observable.

\section{Compatible Observables}
Suppose that we wish to 
simultaneously measure two observables, $\xi$ and $\eta$, of a
microscopic system? Let us assume that we possess an apparatus which is
capable of measuring $\xi$, and another which can measure $\eta$. For instance,
the two observables in question might be the projection in the
$x$- and $z$-directions of the
spin angular momentum  of a spin one-half particle. These could be measured using
appropriate Stern-Gerlach apparatuses (see Sakurai, Sect.~1.1). 
Suppose that we make a measurement of $\xi$, and the system is 
consequently thrown into
one of the eigenstates of $\xi$, $|\xi'\rangle$, with eigenvalue $\xi'$. What
happens if we now make a measurement of $\eta$?  Well, suppose that
the eigenstate $|\xi'\rangle$ is also an eigenstate of $\eta$, with eigenvalue
$\eta'$. In this case, a measurement of $\eta$ will definitely give the
result $\eta'$. A second measurement of $\xi$ will definitely give the
result $\xi'$, and so on. In this sense, we can say that
the observables $\xi$ and $\eta$
{\em simultaneously} have the values $\xi'$ and $\eta'$, respectively. 
Clearly, if all eigenstates of $\xi$ are also eigenstates of $\eta$ then 
it is always possible to make a simultaneous measurement of $\xi$ and $\eta$.
Such observables are termed {\em compatible}. 

Suppose, however, that the eigenstates of $\xi$ are not eigenstates of $\eta$.
Is it still possible to measure both observables simultaneously? Let us again
make an observation of $\xi$ which throws the system into an eigenstate
$|\xi'\rangle$, with eigenvalue $\xi'$.
 We can now make a second observation to determine $\eta$. 
This will throw the system into one of the (many) eigenstates of $\eta$ which
 depend on $|\xi'\rangle$. In principle, each of these eigenstates is
associated with a different result of the measurement. Suppose that the
system is thrown into an eigenstate $|\eta'\rangle$, with the eigenvalue $\eta'$.
Another measurement of $\xi$ will throw the system into one of the (many)
eigenstates of $\xi$ which depend on $|\eta'\rangle$. 
Each eigenstate is again associated with a different possible
result of the measurement. It is clear that if the observables
$\xi$ and $\eta$ do not possess simultaneous eigenstates then if the value
of $\xi$ is known ({\em i.e.}, the system is in an eigenstate of $\xi$)  then the
value of $\eta$ is uncertain ({\em i.e.}, the system is not in an eigenstate
of $\eta$), and {\em vice versa}. We say that the two observables are 
{\em incompatible}.

We have seen that {\em the condition for two observables $\xi$ and $\eta$ to
be simultaneously measurable is that they should possess simultaneous
eigenstates} ({\em i.e.}, every eigenstate of $\xi$ should also be an eigenstate
of $\eta$). Suppose that this is the case. Let a general eigenstate of
$\xi$,  with eigenvalue $\xi'$, also be an eigenstate of
$\eta$, with eigenvalue $\eta'$. It is convenient to denote this
simultaneous eigenstate $|\xi' \eta'\rangle$. We have
\begin{eqnarray}
\xi|\xi' \eta'\rangle &=& \xi' |\xi' \eta'\rangle,\\[0.5ex]
\eta|\xi' \eta'\rangle &=& \eta'|\xi' \eta'\rangle.
\end{eqnarray}
We can left-multiply the first equation by $\eta$, and the second equation
by $\xi$, and then take the difference. The result is
\begin{equation}
(\xi\,\eta -\eta\,\xi) |\xi' \eta'\rangle = |0\rangle
\end{equation}
for each simultaneous eigenstate. 
Recall that the eigenstates of an observable must form a complete set. It follows
that the simultaneous eigenstates of two observables must also form a complete set.
Thus, the above equation implies that
\begin{equation}
(\xi\,\eta -\eta\,\xi) |A\rangle = |0\rangle,
\end{equation}
where $|A\rangle$ is a general ket. The only way that this can
be true is if
\begin{equation}
\xi\,\eta =\eta\,\xi.
\end{equation}
Thus, {\em  the condition for two observables $\xi$ and $\eta$ to be simultaneously
measurable is that they should commute}. 

\section{Uncertainty Relation}
We have seen that if $\xi$ and $\eta$ are two noncommuting observables, then
a determination of the value of $\xi$ leaves the value of $\eta$ uncertain,
and {\em vice versa}. It is possible to quantify this uncertainty. For
a general observable $\xi$, we can define a Hermitian operator
\begin{equation}
\Delta \xi = \xi - \langle \xi \rangle,
\end{equation}
where the expectation value is taken over  the particular physical state under
consideration. It is obvious that the expectation value of $\Delta \xi$ is zero.
The expectation value of $(\Delta \xi)^2 \equiv \Delta \xi\,\Delta \xi$ is termed
the {\em variance} of $\xi$, and is, in general, non-zero. In fact,
it is easily demonstrated that
\begin{equation}
\langle(\Delta \xi)^2\rangle = \langle \xi^2\rangle - \langle \xi\rangle^2.
\end{equation}
The variance of $\xi$ is a measure of the uncertainty in the value of $\xi$ for
the particular state in question ({\em i.e.}, it is a measure of the width of the 
distribution of likely values of $\xi$ about the expectation value). 
If the variance is zero then there is no uncertainty, and a measurement of $\xi$
is bound to give the expectation value, $\langle\xi\rangle$. 

Consider the Schwarz inequality
\begin{equation}
\langle A|A\rangle \langle B|B\rangle \geq |\langle A|B\rangle|^2,
\end{equation}
which is analogous to
\begin{equation}
|{\bf a}|^2 \,|\,{\bf b}|^2 \geq |{\bf a}\cdot {\bf b}|^2
\end{equation}
in Euclidian space. This inequality can be proved by noting that
\begin{equation}
(\langle A| + c^\ast \langle B|) (|A\rangle + c |B\rangle) \geq 0,
\end{equation}
where $c$ is any  complex number. If $c$ takes the special value
$-\langle B|A\rangle/\langle B|B\rangle$ then the above inequality reduces to
\begin{equation}
\langle A|A\rangle \langle B|B\rangle - |\langle A|B\rangle|^2 \geq 0,
\end{equation}
which is the same as the Schwarz inequality. 

Let us substitute 
\begin{eqnarray}
|A\rangle &=& \Delta \xi |~\rangle,\\[0.5ex]
|B\rangle &=& \Delta \eta |~\rangle,
\end{eqnarray}
into the Schwarz inequality, where the blank ket $|~\rangle$ stands for any
general ket. We find
\begin{equation}\label{e2.69}
\langle (\Delta \xi)^2\rangle \langle (\Delta \eta)^2\rangle \geq |\langle
\Delta \xi \,\Delta \eta\rangle |^2,
\end{equation}
where use has been made of the fact that $\Delta \xi$ and $\Delta \eta$ are
Hermitian operators. Note that
\begin{equation}
\Delta \xi \,\Delta \eta= \frac{1}{2} \left[ \Delta \xi, \Delta \eta\right]
+\frac{1}{2} \left\{ \Delta \xi, \Delta \eta\right\},
\end{equation} 
where the {\em commutator}, $\left[ \Delta \xi, \Delta \eta\right]$,
and the {\em anti-commutator}, 
$\left\{ \Delta \xi, \Delta \eta\right\}$, are defined
\begin{eqnarray}
\left[ \Delta \xi, \Delta \eta\right]&\equiv & \Delta\xi \,\Delta
\eta -\Delta \eta\, \Delta \xi,\\[0.5ex]
\left\{ \Delta \xi, \Delta \eta\right\}&\equiv & \Delta \xi\, \Delta \eta + 
\Delta\eta \, \Delta\xi.
\end{eqnarray}
The commutator is clearly {\em anti-Hermitian},
\begin{equation}
(\left[ \Delta \xi, \Delta \eta\right])^{\dag} = (\Delta\xi\,
\Delta\eta-\Delta\eta\,\Delta\xi)^{\dag}
= \Delta\eta\,\Delta\xi-\Delta\xi\,
\Delta\eta = - \left[ \Delta \xi, \Delta \eta\right],
\end{equation}
whereas the anti-commutator is obviously Hermitian. Now, it is easily
demonstrated that the expectation value of a Hermitian operator is a real
number, whereas the expectation value of an anti-Hermitian operator is
a pure imaginary number. It is clear that the right hand side of 
\begin{equation}
\langle\Delta \xi \,\Delta \eta\rangle= \frac{1}{2} 
\langle\left[ \Delta \xi, \Delta \eta\right]\rangle
+\frac{1}{2} \langle\left\{ \Delta \xi, \Delta \eta\right\}\rangle,
\end{equation} 
consists of the sum of a purely real and a purely imaginary number. 
Taking the modulus squared of both sides gives
\begin{equation}
|\langle\Delta \xi \,\Delta \eta\rangle|^2= \frac{1}{4} 
|\langle\left[\xi,  \eta\right]\rangle|^2
+\frac{1}{4} |\langle\left\{ \Delta \xi, \Delta \eta\right\}\rangle|^2,
\end{equation}
where use has been made of $\langle \Delta\xi\rangle = 0$, {\em etc}.
The final term in the above expression is positive definite, so we can write
\begin{equation}\label{e2.75}
\langle (\Delta \xi)^2\rangle \langle (\Delta \eta)^2\rangle \geq \frac{1}{4} 
|\langle\left[ \xi,  \eta\right]\rangle|^2,
\end{equation}
where use has been made of Eq.~(\ref{e2.69}). The above expression is termed the
{\em uncertainty relation}. According to this relation, an exact knowledge
of the value of $\xi$ implies no knowledge whatsoever of the value of $\eta$,
and {\em vice versa}. The one exception to this rule is when $\xi$ and $\eta$
commute, in which case exact knowledge of $\xi$ does not necessarily imply
no knowledge of $\eta$. 

\section{Continuous Spectra}
Up to now, we have studiously avoided dealing with observables  possessing
eigenvalues which lie in a continuous range, rather than having discrete
values. The reason for this is because continuous eigenvalues imply a
ket space of  nondenumerably infinite dimension. Unfortunately, continuous
eigenvalues are unavoidable in quantum mechanics. In fact, the most important
observables of all, namely position and momentum, generally have continuous
eigenvalues. Fortunately, many of the results we obtained previously
for a finite-dimensional ket space with discrete eigenvalues can be
generalized to ket spaces of nondenumerably infinite dimensions.

Suppose that $\xi$ is an observable with continuous eigenvalues. We can still
write the eigenvalue equation as
\begin{equation}
\xi |\xi'\rangle =\xi' |\xi'\rangle.
\end{equation}
But, $\xi'$ can now take a continuous range of values. Let us assume, for
the sake of simplicity, that $\xi'$ can take any value. The orthogonality
condition (\ref{e2.48}) generalizes to
\begin{equation}\label{e2.77}
\langle \xi'|\xi''\rangle = \delta(\xi'-\xi''),
\end{equation}
where $\delta(x)$ denotes the famous Dirac delta-function. Note that there are
clearly a nondenumerably infinite number of mutually orthogonal eigenstates of $\xi$.
Hence, the dimensionality of ket space is nondenumerably infinite. Note, also,
that eigenstates corresponding to a continuous range of eigenvalues {\em cannot}
be normalized so that they have unit norms. In fact, these eigenstates have
{\em infinite} norms: {\em i.e.}, they are infinitely long. This is the major difference
between eigenstates in a finite-dimensional and an infinite-dimensional ket space. 
The extremely useful relation (\ref{e2.50}) generalizes to
\begin{equation}\label{e2.78}
\int d\xi' \,|\xi'\rangle\langle \xi'| = 1.
\end{equation}
Note that a summation over discrete eigenvalues goes over into an integral over
a continuous range of eigenvalues. The eigenstates $|\xi'\rangle$ must form
a complete set if $\xi$ is to be an observable. It follows that any general
ket can be expanded in terms of the $|\xi'\rangle$. In fact, the expansions
(\ref{e2.49a})--(\ref{e2.49c}) generalize to
\begin{eqnarray}
|A\rangle &=& \int d\xi'\,|\xi'\rangle\langle \xi'| A\rangle,\\[0.5ex]
\langle A| &=& \int d\xi'\,\langle A|\xi'\rangle \langle \xi'|,\\[0.5ex]
\langle A|A\rangle &=& \int d\xi'\,\langle A|\xi'\rangle\langle
\xi'|A\rangle = \int d\xi' \,|\langle A|\xi'\rangle|^2.\label{e2.79c}
\end{eqnarray}
These results also follow simply from Eq.~(\ref{e2.78}). We have seen that it is not
possible to normalize the eigenstates $|\xi'\rangle$ such that they have unit norms.
Fortunately, this convenient  normalization is still
 possible for a general state vector.
In fact, according to Eq.~(\ref{e2.79c}), the normalization condition can be written
\begin{equation}
\langle A|A\rangle =\int d\xi' \,|\langle A|\xi'\rangle|^2 = 1.
\end{equation}

We have now studied observables whose eigenvalues can take a discrete number of
values as well as those whose eigenvalues can take any value. There are 
number of other cases
we could look at. For instance, observables whose eigenvalues can only take
a finite range of values, or observables whose eigenvalues take on a
finite range of values plus a set of discrete values. Both of these cases can be 
dealt with  using a fairly straight-forward generalization of the previous
analysis (see Dirac, Cha.\ II and III).

